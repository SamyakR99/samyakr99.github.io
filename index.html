<!DOCTYPE HTML "-//W3C//DTD HTML 4.01 Transitional//EN">
<html lang="en">
  <head>
    <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
    <style type="text/css">
      /* Design Credits: Jon Barron, Deepak Pathak, Saurabh Gupta and Aditya Kusupati*/
      a {
        color: #1772d0;
        text-decoration: none;
      }
  
      a:focus,
      a:hover {
        color: #f09228;
        text-decoration: none;
      }
  
      body,
      td,
      th {
        font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
        font-size: 16px;
        font-weight: 400
      }
  
      heading {
        font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
        font-size: 19px;
        font-weight: 1000
      }
  
      strong {
        font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
        font-size: 16px;
        font-weight: 800
      }
  
      strongred {
        font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
        color: 'red';
        font-size: 16px
      }
  
      sectionheading {
        font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
        font-size: 22px;
        font-weight: 600
      }
  
      pageheading {
        font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
        font-size: 38px;
        font-weight: 400
      }
    </style>
    <!-- <link rel="icon" type="image/png" href="images/W.png"> -->
    <script type="text/javascript" src="js/hidebib.js"></script>
    <title>Samyak Rawlekar</title>

    <meta name="author" content="Samyak Rawlekar">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ”¥</text></svg>">
  </head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr style="padding:0px">
            <td style="padding:2.5%;width:60%;vertical-align:middle">
              <p class="name" style="text-align: center;">
                Samyak Rawlekar
              </p>
              <p>I am a PhD student in the <a href="https://vision.ai.illinois.edu/">Computer Vision and Robotics Laboratory</a> at the <a href="https://illinois.edu/">University of Illinois Urbana-Champaign</a>. 
                I am advised by <a href="https://ece.illinois.edu/about/directory/faculty/n-ahuja"> Prof. Narendra Ahuja </a>. 
          </p>
              <p>I love pixels and have recently been integrating them with text to solve intricate and exciting problems in Computer Vision. Particularly, I focus on using large multi-modal (vision-language) models for multi-task learning. 
          </p>
              <p>Previously, I obtained MS in Computer Engineering from <a href="https://www.nyu.edu/"> New York University </a>, and B.Tech in Electrical Engineering from <a href="https://www.iitdh.ac.in/"> IIT Dharwad </a>.
              </p>
              
              <p style="text-align:center">
                <a target="_blank" href="mailto:samyakrawlekar@gmail.com">E-mail</a> &nbsp;/&nbsp;
                 <a target="_blank" href="https://samyakr99.github.io/files/Samyak_CV.pdf">Resume</a> &nbsp;/&nbsp;
                <a href="https://github.com/SamyakR99">GitHub</a> &nbsp;/&nbsp;
                <a href=https://scholar.google.com/citations?hl=en&user=4Jp_SN4AAAAJ&view_op=list_works&sortby=pubdate">Scholar</a> &nbsp;/&nbsp;
                <a href="https://twitter.com/SamyakR23">X</a> &nbsp;/&nbsp;
                <a href="https://bsky.app/profile/samyakr.bsky.social">Bsky</a> &nbsp;/&nbsp;
                <a href="gallery.html">Gallery</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <img style="width:100%;max-width:100%" alt="profile photo" src="images/Web_img1.jpg">
            </td>
          </tr>
        </table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <h2>Selected Publications</h2>
              
            </td>
          </tr>
        </tbody></table>
      
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
      
        <tr>
          <td style="padding:2.5%;width:40%;vertical-align:top;min-width:120px">
            <img src="images/PosCoOp.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
          </td>
          <td style="padding:2.5%;width:60%;vertical-align:top">
            <h3>PositiveCoOp: Rethinking Prompting Strategies for Multi-Label Recognition with Partial Annotations</h3>
            <br>
            <strong>Samyak Rawlekar</strong>, <a href="https://shubhangb97.github.io/">Shubhang Bhatnagar </a>, <a href="https://en.wikipedia.org/wiki/Narendra_Ahuja"> Narendra Ahuja </a>

            <br>
            <em> <a href="https://wacv2025.thecvf.com/">WACV </a></em> 2025</em>
            <br>
            
            <a href="javascript:toggleblock('PosCoOp_abs')">abstract</a> /
             <a href="https://samyakr99.github.io/PositiveCoOp">project page</a> /
            <a href="https://arxiv.org/pdf/2409.08381">paper</a>

            <!-- <a href="coming soon">video</a> / 
             -->
            <br>

              <p align="justify"> <i id="PosCoOp_abs">Vision-language models (VLMs) like CLIP have been adapted for Multi-Label Recognition (MLR) with partial annotations by leveraging prompt-learning, where positive and negative prompts are learned for each class to associate their embeddings with class presence or absence in the shared vision-text feature space. While this approach improves MLR performance by relying on VLM priors, we hypothesize that learning negative prompts may be suboptimal, as the datasets used to train VLMs lack image-caption pairs explicitly focusing on class absence. To analyze the impact of positive and negative prompt learning on MLR, we introduce PositiveCoOp and NegativeCoOp, where only one prompt is learned with VLM guidance while the other is replaced by an embedding vector learned directly in the shared feature space without relying on the text encoder. Through empirical analysis, we observe that negative prompts degrade MLR performance, and learning only positive prompts, combined with learned negative embeddings (PositiveCoOp), outperforms dual prompt learning approaches. Moreover, we quantify the performance benefits that prompt-learning offers over a simple vision-features-only baseline, observing that the baseline displays strong performance comparable to dual prompt learning approach (DualCoOp), when the proportion of missing labels is low, while requiring half the training compute and 16 times fewer parameters.</i></p>
          </td>
        </tr>

        <tr>
          <td style="padding:2.5%;width:40%;vertical-align:top;min-width:120px">
            <img src="images/MLR_GCN.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
          </td>
          <td style="padding:2.5%;width:60%;vertical-align:top">
            <h3>Improving Multi-label Recognition using Class Co-Occurrence Probabilities</h3>
            <br>
            <strong>Samyak Rawlekar*</strong>, <a href="https://shubhangb97.github.io/">Shubhang Bhatnagar* </a>,  Vishnuvardhan Pogunulu Srinivasulu, <a href="https://en.wikipedia.org/wiki/Narendra_Ahuja"> Narendra Ahuja </a>

            <br>
            <em> <a href="https://sites.google.com/view/cvpr-metafood-2024">CVPRW </a></em> 2024,<em> <a href="https://icpr2024.org/">ICPR </a> 2024 (Oral Top-5%)</em>
            <br>
            
            <a href="javascript:toggleblock('MLR1_abs')">abstract</a> /
             <a href="https://shubhangb97.github.io/MLR_gcn">project page</a> /
            <a href="https://arxiv.org/abs/2404.16193">paper</a>

            <!-- <a href="coming soon">video</a> / 
             -->
            <br>

              <p align="justify"> <i id="MLR1_abs">Multi-label Recognition (MLR) involves the identification of multiple objects within an image. 
                To address the additional complexity of this problem, recent works have leveraged information from vision-language models (VLMs) trained on large text-images datasets for 
                the task. These methods learn an independent classifier for each object (class), overlooking correlations in their occurrences. Such co-occurrences can be captured from the
                 training data as conditional probabilities between a pair of classes. We propose a framework to extend the independent classifiers by incorporating the 
                 co-occurrence information for object pairs to improve the performance of independent classifiers. We use a Graph Convolutional Network (GCN) to enforce the 
                 conditional probabilities between classes, by refining the initial estimates derived from image and text sources obtained using VLMs. We validate our method on four MLR
                  datasets, where our approach outperforms all state-of-the-art methods.</i></p>
          </td>
        </tr>

    <tr>
          <td style="padding:2.5%;width:40%;vertical-align:top;min-width:120px">
            <img src="images/S30.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
          </td>
          <td style="padding:2.5%;width:60%;vertical-align:top">
            <h3>S3O: A Dualâ€‘Phase Approach for Reconstructing Dynamic Shape and Skeleton</h3>
            <br>
            <a href="https://haoz19.github.io/">Hao Zhang* </a>, <a href="https://fangli333.github.io/">Fang Li* </a><strong>Samyak Rawlekar*</strong>, <a href="https://en.wikipedia.org/wiki/Narendra_Ahuja"> Narendra Ahuja </a>

            <br>
            <em> <a href="https://icml.cc/Conferences/2024">ICML </a></em> 2024
            <br>
            
            <a href="javascript:toggleblock('S30 Abs')">abstract</a> /
            <a href="https://arxiv.org/abs/2405.12607">paper</a>

            <!-- <a href="coming soon">video</a> / 
             -->
            <br>

              <p align="justify"> <i id="S30 Abs">Reconstructing dynamic articulated objects from a singular monocular video is challenging, requiring joint estimation of shape, motion, and camera parameters from limited views. Current methods typically demand extensive computational resources and training time, and require additional human annotations such as predefined parametric models, camera poses, and key points, limiting their generalizability. We propose Synergistic Shape and Skeleton Optimization (S3O), a novel two-phase method that forgoes these prerequisites and efficiently learns parametric models including visible shapes and underlying skeletons. Conventional strategies typically learn all parameters simultaneously, leading to interdependencies where a single incorrect prediction can result in significant errors. In contrast, S3O adopts a phased approach: it first focuses on learning coarse parametric models, then progresses to motion learning and detail addition. This method substantially lowers computational complexity and enhances robustness in reconstruction from limited viewpoints, all without requiring additional annotations. To address the current inadequacies in 3D reconstruction from monocular video benchmarks, we collected the PlanetZoo dataset. Our experimental evaluations on standard benchmarks and the PlanetZoo dataset affirm that S3O provides more accurate 3D reconstruction, and plausible skeletons, and reduces the training time by approximately 60% compared to the state-of-the-art, thus advancing the state of the art in dynamic object reconstruction..</i></p>
          </td>
        </tr>

        <tr>
          <td style="padding:2.5%;width:40%;vertical-align:top;min-width:120px">
            <img src="images/LIMR.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
          </td>
          <td style="padding:2.5%;width:60%;vertical-align:top">
            <h3>LIMR: Learning Implicit Representation for Reconstructing Articulated Objects</h3>
            <br>
            <a href="https://haoz19.github.io/">Hao Zhang* </a>, <a href="https://fangli333.github.io/">Fang Li* </a><strong>Samyak Rawlekar*</strong>, <a href="https://en.wikipedia.org/wiki/Narendra_Ahuja"> Narendra Ahuja </a>

            <br>
            <em> <a href="https://iclr.cc/Conferences/2024">ICLR </a></em> 2024
            <br>
            
            <a href="javascript:toggleblock('LIMR Abs')">abstract</a> /
            <a href="https://arxiv.org/abs/2401.08809">paper</a>

            <!-- <a href="coming soon">video</a> / 
             -->
            <br>

              <p align="justify"> <i id="LIMR Abs">3D Reconstruction of moving articulated objects without additional information about object structure is a challenging problem. Current methods overcome such challenges by employing category-specific skeletal models. Consequently, they do not generalize well to articulated objects in the wild. We treat an articulated object as an unknown, semi-rigid skeletal structure surrounded by nonrigid material (e.g., skin). Our method simultaneously estimates the visible (explicit) representation (3D shapes, colors, camera parameters) and the implicit skeletal representation, from motion cues in the object video without 3D supervision. Our implicit representation consists of four parts. (1) Skeleton, which specifies how semi-rigid parts are connected. (2) \textcolor{black}{Skinning Weights}, which associates each surface vertex with semi-rigid parts with probability. (3) Rigidity Coefficients, specifying the articulation of the local surface. (4) Time-Varying Transformations, which specify the skeletal motion and surface deformation parameters. We introduce an algorithm that uses physical constraints as regularization terms and iteratively estimates both implicit and explicit representations. Our method is category-agnostic, thus eliminating the need for category-specific skeletons, we show that our method outperforms state-of-the-art across standard video datasets.</i></p>
          </td>
        </tr>


    



        
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td><br>
              <p align="right">
                <font size="2">
                  <a href="https://jonbarron.info">Template</a>
                </font>
              </p>
            </td>
          </tr>
        </table>
        <script xml:space="preserve" language="JavaScript">
          hideblock('PosCoOp_abs');
        </script>
       <script xml:space="preserve" language="JavaScript">
          hideblock('MLR1_abs');
        </script>
         <script xml:space="preserve" language="JavaScript">
          hideblock('pal_abs');
        </script>
        <script xml:space="preserve" language="JavaScript">
          hideblock('pcfb_abs');
        </script>
        <script xml:space="preserve" language="JavaScript">
          hideblock('qr_abs');
        </script>
        <script xml:space="preserve" language="JavaScript">
          hideblock('adapter_abs');
        </script>
        <br>
        <br>
        <br>




        <!--
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:2.5%;width:100%;vertical-align:middle">
              <h2>Intel Projects</h2>
              <p>Besides my work on the RealSense depth sensors and the publications above, a sampling of my publicly disclosed work
              </p>
            </td>
          </tr>
        </table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">

          {% for post in site.posts %}
          {% for cat in post.categories %}
          {% if cat == 'Intel' %}
          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="/tn{{post.image}}" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>{{post.title}}</h3>
              <br>
              <em>{{post.categories}} {{post.course}}</em>
              <br>
              {{ post.date | date: "%Y-%m-%d" }}

              <br>
              {% if post.website %}
              <a href="{{post.website}}">website</a> /
              {% endif %}
              {% if post.paper %}
              <a href="{{post.paper}}">paper</a> /
              {% endif %}
              {% if post.patent %}
              <a href="{{post.patent}}">patent</a> /
              {% endif %}
              {% if post.patent2 %}
              <a href="{{post.patent2}}">patent #2</a> /
              {% endif %}
              {% if post.patent3 %}
              <a href="{{post.patent3}}">patent #3</a> /
              {% endif %}
              {% if post.video %}
              <a href="{{post.video}}">video</a> /
              {% endif %}
              {% if post.video2 %}
              <a href="{{post.video2}}">video #2</a> /
              {% endif %}
              {% if post.code %}
              <a href="{{post.code}}">code</a> /
              {% endif %}
              {% if post.poster %}
              <a href="{{post.poster}}">poster</a> /
              {% endif %}
              {% if post.slides %}
              <a href="{{post.slides}}">slides</a> /
              {% endif %}
              <p></p>
              {{ post.excerpt }}
            </td>
          </tr>
          {% endif %}
          {% endfor %}
          {% endfor %}
        </table>-->
        
</body>

</html>



