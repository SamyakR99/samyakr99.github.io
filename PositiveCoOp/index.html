
<!DOCTYPE html>
<html>

    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>Rethinking Prompting Strategies for Multi-Label Recognition with Partial
Annotations</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">



    <meta property="og:type" content="website" />

    <meta property="og:title" content="Rethinking Prompting Strategies for Multi-Label Recognition with Partial
Annotations" />

    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Rethinking Prompting Strategies for Multi-Label Recognition with Partial
Annotations" />


<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>❄️</text></svg>">

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
    <script src="js/video_comparison.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                <b>Rethinking Prompting Strategies for Multi-Label Recognition with Partial
Annotations</b></br> 
                
                Winter Conference on Applications of Computer Vision (WACV 2025)  
                
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://samyakr99.github.io/">
                            Samyak Rawlekar<sup>1</sup> 
                        </a>
                    </li>
                    <li>
                        <a href="https://shubhangb97.github.io/">
                             Shubhang Bhatnagar <sup>1</sup> 
                        </a>
                    </li>
                    <li>
                        <a href="https://ece.illinois.edu/about/directory/faculty/n-ahuja">
                            Narendra Ahuja 
                        </a><sup>1</sup>
                    </li>
                    
                   
                    </br>
                    <li>
                        <sup>1</sup><a href="https://illinois.edu/">
                            University of Illinois at Urbana-Champaign
                        </a>
                    </li>
                </ul>
            </div>
        </div>


        <div class="row">
                <div class="col-md-4 col-md-offset-2 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="https://illinois.edu/">
                           <image src="img/uiuc_logo.png" height="60px">
                                
                            </a>
                        </li>
                        <li>
                            <a href="https://arxiv.org/pdf/2409.08381">
                            <image src="img/wacv_paper_snap.png" height="60px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="">
                            <image src="img/poster_snapshot.png" height="60px">
                                <h4><strong>Poster</strong></h4>
                            </a>
                        </li>
			<li>
                            <a href="">
                            <image src="img/MLR_food_snapshot.png" height="60px">
                                <h4><strong>Slides</strong></h4>
                            </a>
                        </li>
			<li>
                            <a href="">
                            <image src="img/code.png" height="60px">
                                <h4><strong>Code \n (coming soon) </strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://wacv2025.thecvf.com/">
                           <image src="img/WACV-2025-Logo.png" height="60px">
                                
                            </a>
                        </li>
                        <!-- <li>
                            <a href="img/LongDistanceGestureRecognition_IROS_final.pdf">
                            <image src="img/slides_snapshot.PNG" height="60px">
                                <h4><strong>Slides</strong></h4>
                            </a>
                        </li> -->
                    </ul>
                </div>
        </div>


         <!--
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <video id="v0" width="100%" autoplay loop muted controls>
                  <source src="img/demo_final_noaudio.mp4" type="video/mp4" />
                </video>
                <div class="text-center">
                <strong>A demonstration of our method used to control a mobile Robot</strong>
            </div>
						</div>
        </div>
        -->

    <div class="text-center">
	    <img src="./img/teaser.png" width="65%">
	</div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
                    Vision-language models (VLMs) like CLIP have been adapted for Multi-Label Recognition (MLR) with partial annotations by leveraging prompt-learning, where positive and negative prompts are learned for each class to associate their embeddings with class presence or absence in the shared vision-text feature space. While this approach improves MLR performance by relying on VLM priors, we hypothesize that learning negative prompts may be suboptimal, as the datasets used to train VLMs lack image-caption pairs explicitly focusing on class absence. To analyze the impact of positive and negative prompt learning on MLR, we introduce PositiveCoOp and NegativeCoOp, where only one prompt is learned with VLM guidance while the other is replaced by an embedding vector learned directly in the shared feature space without relying on the text encoder. Through empirical analysis, we observe that negative prompts degrade MLR performance, and learning only positive prompts, combined with learned negative embeddings (PositiveCoOp), outperforms dual prompt learning approaches. Moreover, we quantify the performance benefits that prompt-learning offers over a simple vision-features-only baseline, observing that the baseline displays strong performance comparable to dual prompt learning approach (DualCoOp), when the proportion of missing labels is low, while requiring half the training compute and 16 times fewer parameters.
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Motivation
                </h3>
                <!--<div class="text-center">
                    <img src="./img/motivation_diagram.svg" width="100%">
                </div>-->
                <br><br>
                <div class="text-justify">
                    Prompt learning is a popular approach for recognition task with vision-language models (VLMs), as it helps take advantage of 
the latent knowledge in the text encoder to learn prompts whose embeddings in the shared vision-language embedding space correspond to the presence of a class. 
Several works have extended prompt learning to multi-label recognition by learning dual prompts: a positive and a negative prompt,
to detect the presence and absence of a class in the image, respectively. However, a closer analysis of VLMs reveals that they have been trained on paired image-caption datasets where captions 
correspond to the presence of objects rather than their absence. This raises questions about the guidance from VLMs for learning negative prompts in such approaches. 
To this end, we conduct a thorough empirical study to evaluate the contribution of VLM guidance in learning both positive and negative prompts.
                
                
                
			</div>

            </div>
        </div>
        <br>
        <br>
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Proposed Model
                </h3>
            <div class="text-justify">
		    VLM-based MLR approaches like DualCoOp propose learning both positive and negative prompts using CLIP's guidance: one for class presence and one for class absence. 
		    In PositiveCoOp (NegativeCoOp), for a given class <i>j</i>, only the positive (negative) prompt <b>t<sub>j,+</sub></b> (<b>t<sub>j,-</sub></b>) is learned through CLIP, 
		    while the negative (positive) prompt is replaced by a learned text embedding <b>r<sub>j,-</sub></b> (<b>r<sub>j,+</sub></b>) in the feature space, independent of CLIP's text encoder.
		    For both PositiveCoOp and NegativeCoOp, we obtain the final predictions <b>p<sub>i,j,+</sub></b> and <b>p<sub>i,j,-</sub></b> by calculating the cosine similarity of the image features with the embedding of the positive text prompt <b>r<sub>j,+</sub></b> and learned text embedding <b>r<sub>j,-</sub></b>. 
		    This is then aggregated using the class-specific feature aggregation strategy following DualCoOp. Only the text embeddings and the prompts are trained using the widely used Asymmetric Loss.
                <br><br>
                <div class="text-center">
                    <img src="./img/main.svg" width="95%">
                </div>
                
			</div>

            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Results and Analysis
                </h3>
            
            <div class="text-justify">
                A. The results of MLR with partial annotations on COCO and VOC2007 demonstrate that the performance of the prompting-based approaches follows the order: Positive CoOp > DualCoOp ≈ Baseline > NegativeCoOp.
                <br><br>
                <div class="text-center">
                    <img src="./img/result1.png" width="95%">
                </div>
                
			</div>

            <div class="text-justify">
                
		    B. The comparison of training parameters and GPU hours for the exsiting methods with the Baseline, PostitiveCoOp and NegativeCoOp reveal that the Baseline uses significantly fewer parameters and
GPU hours than all other setups, while PositiveCoOp and NegativeCoOp require about half the parameters compared to DualCoOp.
                <br><br>
                <div class="text-center">
                    <img src="./img/result2_compute.png" width="50%">
                </div>
                
			</div>

            <div class="text-justify">
               
		    C. We compare the average similarity between pairs of positive features and
pairs of positive and negative on 80 classes of COCO dataset for
two scenarios a) When we use a only one prompt and b) Using 85 default prompt templates for ImageNet. We observe that
the similarity score between positive-positive prompt is close to
positive-negative, implying that CLIP projects positive and negative prompts very closely in the feature space.
                <br><br>
                <div class="text-center">
                    <img src="./img/result3_prompt_sim.png" width="50%">
                </div>
                
			</div>

            </div>
        </div>
       

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
                        @article{rawlekar2024rethinking,
			  title={Rethinking Prompting Strategies for Multi-Label Recognition with Partial Annotations},
			  author={Rawlekar, Samyak and Bhatnagar, Shubhang and Ahuja, Narendra},
			  journal={arXiv preprint arXiv:2409.08381},
			  year={2024}
			}
                    </textarea>
                </div>
            </div>
        </div>

        <div class="row">
            
                <p class="text-justify">
                The website template was borrowed from <a href="http://mgharbi.com/">Michaël Gharbi</a> and <a href="https://dorverbin.github.io/refnerf">Ref-NeRF</a>.
                </p>
            
        </div>
    </div>
</body>
</html>
